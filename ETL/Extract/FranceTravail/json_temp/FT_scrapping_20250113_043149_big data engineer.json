[
    {
        "source": "France Travail",
        "link": "https://candidat.francetravail.fr/offres/recherche/detail/0648839",
        "title": "Data Engineer / Big Data Developement Hadoop et Spark F/H - Système, réseaux, données (H/F)",
        "company": "DGTL PERFORMANCE ",
        "location": "06",
        "remote": null,
        "publication_date": "2025-01-13",
        "details": {
            "TypeContract": "CDI",
            "Salary": "Salaire brut : A partir de 35 k€ brut annuel",
            "Experience": "Expérience exigée de 5 An(s)",
            "Level": null
        },
        "company_data": {
            "sector": "Conseil pour les affaires et autres conseils de gestion",
            "company_size": "DGTL / Signe + est le facilitateur pour tous les acteurs qui recherchent des ressources ou des missions DATA.\n\n\n\nSpécialiste du marché Data et BI, nous intervenons dans toute la France comme à l'étranger ; en sous-traitance, pré-embauche, recrutement,  commercial,  salarial, etc.\n\n\n\nDepuis 2018, nous accompagnons nos clients avec proximité, juste prix et préoccupation éthique de tous les instants.\n\n\n\nhttps://www.dgtl-performance.com",
            "turnover_in_millions": null
        },
        "description": "Descriptif du poste:\n\nIntégrer l'équipe KARMA USA pour participer à la maintenance et au développement des nouveaux besoins métiers. KARMA est le système de Revenue Management du groupe Air France KLM\n\n\n\nNécessite une bonne connaissance de l'environnement Hadoop et une expérience certaine en développement Hadoop Map Reduce et Spark Java.\n\nIntégration d'une équipe de 8 personnes\n\nProfil recherché:\n\nGrade B+ \n\n5ans xp minimum \n\nLoc : Valbonne - Sur site 1 x par semaine \n\n\n\nPrésence sur site (Sophia Antipolis) obligatoire 1x par semaine.\n\nPrésence sur site (Sophia Antipolis) obligatoire 3x par semaine pendant les 3 premiers mois (on boarding)"
    }
, {
    "source": "France Travail",
    "link": "https://candidat.francetravail.fr/offres/recherche/detail/0645620",
    "title": "Data Engineer Expérimenté(e) - Industrie & Services - Clermont-Ferrand (H/F)",
    "company": null,
    "location": "CLERMONT FERRAND 63",
    "remote": null,
    "publication_date": "2025-01-12",
    "details": {
        "TypeContract": "CDI",
        "Salary": null,
        "Experience": "Expérience exigée de 2 An(s)",
        "Level": null
    },
    "company_data": {
        "sector": null,
        "company_size": null,
        "turnover_in_millions": null
    },
    "description": "Au sein du vertical Industrie & Services de Clermont-Ferrand, nous accompagnons des grandes entreprises dans leurs projets de data.\nNous sommes notamment en charge de créer des nouvelles solutions de data dans le cadre d\\'activités de supply chain ou encore de manufacturing.\nDans ce cadre, nous recrutons des Data Engineer Expérimenté(e) afin d\\'intégrer nos équipes.\nEn rejoignant notre entreprise, vous avez la chance de pogresser à une vistesse remarquable et d\\'avoir un réel impact sur les projets de nos clients.\n \nVotre rôle et vos missions :\nVous remplissez les missions suivantes (non-exhaustif) :\n * Vous êtes participez au développement et à la maintenance de plateformes data ;\n * Vous partagez votre énergie contagieuse et votre passion de l'excellence technique aux équipes ;\n * Vous êtes force de proposition et apportez une vision nouvelle et innovante ;\n * Vous collaborez avec des esprits brillants pour transformer les idées en produits concrets ;\n * Vous brisez les barrières et repoussez les limites de ce qui est possible dans le monde de la data.\nTechnologies : Databricks, Snowflake, Python, PySpark, SQL, Gitlab CI/CD, Azure DevOps, etc.\nVos compétences techniques :\n * Indispensable : vous avez à minima 3 ans d\\'expérience sur des technologies de data \\\"modernes\\\" telles que Databricks ou encore Snowflake par exemple.\n * Indispensable également : vous maîtrisez les outils d\\'industrialisation et / ou d\\'automatisation tels que Gitlab CI/CD, Azure DevOps, etc.\n * Est un \\\"+\\\" : vous connaissez les secteurs de la supply chain et / ou du manufacturing.\n \nVos compétences comportementales :\n * Vous savez rechercher des solutions et faire preuve d\\'initiative :\nVous ne fermez pas les yeux sur les problèmes que vous détectez. Vous alertez et proposez des solutions. Voir même, selon votre séniorité, vous corrigez directement les problèmes.\n * Vous faîtes preuve d\\'ouverture professionnelle :\nVous êtes toujours motivé(e) pour acquérir des nouvelles compétences. Vous n\\'êtes pas dépassé(e) par un nouveau sujet, mais vous prenez bien le temps de vous renseigner afin d\\'être en maîtrise.\n * Vous avez le sens du collectif :\nVous partagez avec transparence, les difficultés que vous rencontrez à vos collègues afin d\\'obtenir de l\\'aide. Vous restez attentif/ve aux besoins de vos collègues afin de les aider lorsque vous êtes légitime pour...."
}, {
    "source": "France Travail",
    "link": "https://candidat.francetravail.fr/offres/recherche/detail/0645483",
    "title": "Data Engineer Expérimenté H/F",
    "company": null,
    "location": "IVRY SUR SEINE 94",
    "remote": null,
    "publication_date": "2025-01-12",
    "details": {
        "TypeContract": "CDI",
        "Salary": null,
        "Experience": "Débutant accepté",
        "Level": null
    },
    "company_data": {
        "sector": "Activités des agences de placement de main-d'œuvre",
        "company_size": null,
        "turnover_in_millions": null
    },
    "description": "Notre client est une société au service d\\'un des leaders de la grande distribution française. Vous rejoignez une entreprise de 800 Collaborateurs qui travaillent au quotidien sur les enjeux du retail multicanal et multiformat.\nMissions : \n * Concevoir, développer et maintenir des pipelines de données robustes et évolutifs en utilisant les services et les outils GCP tels que BigQuery, Pub/Sub, Cloud Storage, Cloud Composer etc.\n * Mettre en place des processus de transformation et de contrôle des données pour garantir leur qualité et leur cohérence. Utiliser des langages de programmation tels que SQL et Python et des outils tels que DBT pour effectuer des opérations de transformation et d\\'enrichissement des données, d\\'agrégation pour alimenter les cas d\\'usages métier,\n * Analyser les performances des pipelines de données, identifier les dérives et proposer des améliorations pour optimiser les temps de traitement et assurer une scalabilité adéquate,\n * Rédiger la documentation technique (spécifications, documents d\\'exploitation.) afin d\\'assurer la capitalisation,\n * Mettre en place des pipelines CI/CD pour automatiser le déploiement, les tests, et la gestion des développements,\n * Travailler en étroite collaboration avec les squads big data, les équipes de l\\'intégrateur et de la DSI ainsi que les métiers pour comprendre leurs besoins en matière de données,\n * Contribuer au support de production, à la correction des incidents et des anomalies, implémenter les évolutions fonctionnelles et techniques pour garantir la stabilité des traitements en production.\n* Télétravail de deux jours,\n * Un poste évolutif,\n * Un environnement de travail bienveillant et agréable.\nVous cumulez un minimum de 4 ans d\\'expérience, avec une expertise spécifique sur GCP sur des projets data intégrant d\\'importants volumes de données : \n * Solides compétences en développement et en mise en œuvre de pipelines de données sur GCP, en utilisant des services tels que Cloud Storage, BigQuery, Pub/Sub, Cloud Composer et des outils tels que DBT,\n * Maîtrise des langages de programmation tels que Python et SQL,\n * Maîtrise des pratiques DevOps, du CI/CD et de l\\'IaC,\n * Avoir travaillé dans le monde du retail est un réel plus...."
}]