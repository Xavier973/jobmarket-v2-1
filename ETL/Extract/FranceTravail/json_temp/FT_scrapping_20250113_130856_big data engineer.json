[
    {
        "source": "France Travail",
        "link": "https://candidat.francetravail.fr/offres/recherche/detail/186NLHC",
        "title": "Expert Data H/F (H/F)",
        "company": "ITEA ",
        "location": "67",
        "remote": null,
        "publication_date": "2025-01-13",
        "details": {
            "TypeContract": "CDI",
            "Salary": "Salaire brut : selon profil",
            "Experience": "3 An(s)",
            "Level": null
        },
        "company_data": {
            "sector": "Conseil en systèmes et logiciels informatiques",
            "company_size": "20 à 49 salariés",
            "turnover_in_millions": null
        },
        "description": "Plus qu'un métier, itéa a pour vocation de déceler et révéler les bons profils, avec une curiosité qui va souvent au-delà des compétences, et parfois entre les lignes d'un CV.\n\nAujourd'hui, nous recherchons pour l'un de nos clients, un Expert Data H/F et vous présentons les points clés du poste :\n\n- Poste en CDI\n- Basé à Strasbourg\n- Secteur public\n- Modélisation et architecture des données : Expertise dans la création et la gestion de modèles de données complexes (conceptuel, logique, physique).\n\nTechnologies et outils :\n- Expérience souhaitée sur des plateformes de virtualisation des données comme Denodo, Tibco ou Informatica.\n- Maîtrise des environnements et des technologies de stockage et de traitement des données (bases relationnelles, entrepôts de données, Data Lake, etc.).\n- Bonne connaissance des ETL/ELT, et d'outils BI comme BO, Power BI ou Superset.\n- Gouvernance des données : Compétence en mise en œuvre de processus liés à la qualité des données, la sécurité, et la gestion des métadonnées.\n\nVos missions seront de :\n\n- Concevoir et superviser l'architecture globale des flux de données au sein de la collectivité.\n- Participer activement à la gouvernance des données et accompagner les chefs de projet dans leurs choix technologiques.\n- Proposer des solutions innovantes pour optimiser la gestion des données et leur exploitation.\n- Travailler en collaboration avec le service AI pour garantir une cohérence entre l'architecture data et l'architecture globale de la collectivité.\n- Contribuer à la stratégie Data Mesh et à l'amélioration continue des infrastructures et procédures naissantes.\n\nEt vous ?\n\n- Vous êtes diplômé Bac + 5 en informatique\n- Vous avez déjà une expérience de 3 à 7 ans sur un poste similaire\n- Vous êtes spécialiste dans la création et la gestion de modèles de données complexes\n- Vous maîtrisez des outils BI comme BO, Power BI ou Superset\n- Vous maîtrisez des outils de traitement de données et de stockage (Data Lake) et des plateformes de virtualisation (Denodo, Tibco ou Informatica)\n- Vous avez des connaissances au sujet de la sécurisation des données, leurs traitements et la gestion des métadonnées\n- Vous êtes capable de simplifier des concepts techniques \n- Vous avez une bonne capacité d'analyse, de communication et d'esprit d'initiative\n \n\nVous vous retrouvez dans ces critères, vous avez le goût du travail bien accompli et appréciez la diversité des tâches, prenez le temps de postuler afin d'échanger avec nous sur cette opportunité.\n\nDans le fond, c'est toujours une belle histoire de rencontres, de connexions et de valeurs."
    }
, {
    "source": "France Travail",
    "link": "https://candidat.francetravail.fr/offres/recherche/detail/0648839",
    "title": "Data Engineer / Big Data Developement Hadoop et Spark F/H - Système, réseaux, données (H/F)",
    "company": "DGTL PERFORMANCE ",
    "location": "06",
    "remote": null,
    "publication_date": "2025-01-13",
    "details": {
        "TypeContract": "CDI",
        "Salary": "Salaire brut : A partir de 35 k€ brut annuel",
        "Experience": "Expérience exigée de 5 An(s)",
        "Level": null
    },
    "company_data": {
        "sector": "Conseil pour les affaires et autres conseils de gestion",
        "company_size": "DGTL / Signe + est le facilitateur pour tous les acteurs qui recherchent des ressources ou des missions DATA.\n\n\n\nSpécialiste du marché Data et BI, nous intervenons dans toute la France comme à l'étranger ; en sous-traitance, pré-embauche, recrutement,  commercial,  salarial, etc.\n\n\n\nDepuis 2018, nous accompagnons nos clients avec proximité, juste prix et préoccupation éthique de tous les instants.\n\n\n\nhttps://www.dgtl-performance.com",
        "turnover_in_millions": null
    },
    "description": "Descriptif du poste:\n\nIntégrer l'équipe KARMA USA pour participer à la maintenance et au développement des nouveaux besoins métiers. KARMA est le système de Revenue Management du groupe Air France KLM\n\n\n\nNécessite une bonne connaissance de l'environnement Hadoop et une expérience certaine en développement Hadoop Map Reduce et Spark Java.\n\nIntégration d'une équipe de 8 personnes\n\nProfil recherché:\n\nGrade B+ \n\n5ans xp minimum \n\nLoc : Valbonne - Sur site 1 x par semaine \n\n\n\nPrésence sur site (Sophia Antipolis) obligatoire 1x par semaine.\n\nPrésence sur site (Sophia Antipolis) obligatoire 3x par semaine pendant les 3 premiers mois (on boarding)"
}, {
    "source": "France Travail",
    "link": "https://candidat.francetravail.fr/offres/recherche/detail/0645620",
    "title": "Data Engineer Expérimenté(e) - Industrie & Services - Clermont-Ferrand (H/F)",
    "company": null,
    "location": "CLERMONT FERRAND 63",
    "remote": null,
    "publication_date": "2025-01-12",
    "details": {
        "TypeContract": "CDI",
        "Salary": null,
        "Experience": "Expérience exigée de 2 An(s)",
        "Level": null
    },
    "company_data": {
        "sector": null,
        "company_size": null,
        "turnover_in_millions": null
    },
    "description": "Au sein du vertical Industrie & Services de Clermont-Ferrand, nous accompagnons des grandes entreprises dans leurs projets de data.\nNous sommes notamment en charge de créer des nouvelles solutions de data dans le cadre d\\'activités de supply chain ou encore de manufacturing.\nDans ce cadre, nous recrutons des Data Engineer Expérimenté(e) afin d\\'intégrer nos équipes.\nEn rejoignant notre entreprise, vous avez la chance de pogresser à une vistesse remarquable et d\\'avoir un réel impact sur les projets de nos clients.\n \nVotre rôle et vos missions :\nVous remplissez les missions suivantes (non-exhaustif) :\n * Vous êtes participez au développement et à la maintenance de plateformes data ;\n * Vous partagez votre énergie contagieuse et votre passion de l'excellence technique aux équipes ;\n * Vous êtes force de proposition et apportez une vision nouvelle et innovante ;\n * Vous collaborez avec des esprits brillants pour transformer les idées en produits concrets ;\n * Vous brisez les barrières et repoussez les limites de ce qui est possible dans le monde de la data.\nTechnologies : Databricks, Snowflake, Python, PySpark, SQL, Gitlab CI/CD, Azure DevOps, etc.\nVos compétences techniques :\n * Indispensable : vous avez à minima 3 ans d\\'expérience sur des technologies de data \\\"modernes\\\" telles que Databricks ou encore Snowflake par exemple.\n * Indispensable également : vous maîtrisez les outils d\\'industrialisation et / ou d\\'automatisation tels que Gitlab CI/CD, Azure DevOps, etc.\n * Est un \\\"+\\\" : vous connaissez les secteurs de la supply chain et / ou du manufacturing.\n \nVos compétences comportementales :\n * Vous savez rechercher des solutions et faire preuve d\\'initiative :\nVous ne fermez pas les yeux sur les problèmes que vous détectez. Vous alertez et proposez des solutions. Voir même, selon votre séniorité, vous corrigez directement les problèmes.\n * Vous faîtes preuve d\\'ouverture professionnelle :\nVous êtes toujours motivé(e) pour acquérir des nouvelles compétences. Vous n\\'êtes pas dépassé(e) par un nouveau sujet, mais vous prenez bien le temps de vous renseigner afin d\\'être en maîtrise.\n * Vous avez le sens du collectif :\nVous partagez avec transparence, les difficultés que vous rencontrez à vos collègues afin d\\'obtenir de l\\'aide. Vous restez attentif/ve aux besoins de vos collègues afin de les aider lorsque vous êtes légitime pour...."
}, {
    "source": "France Travail",
    "link": "https://candidat.francetravail.fr/offres/recherche/detail/0645483",
    "title": "Data Engineer Expérimenté H/F",
    "company": null,
    "location": "IVRY SUR SEINE 94",
    "remote": null,
    "publication_date": "2025-01-12",
    "details": {
        "TypeContract": "CDI",
        "Salary": null,
        "Experience": "Débutant accepté",
        "Level": null
    },
    "company_data": {
        "sector": "Activités des agences de placement de main-d'œuvre",
        "company_size": null,
        "turnover_in_millions": null
    },
    "description": "Notre client est une société au service d\\'un des leaders de la grande distribution française. Vous rejoignez une entreprise de 800 Collaborateurs qui travaillent au quotidien sur les enjeux du retail multicanal et multiformat.\nMissions : \n * Concevoir, développer et maintenir des pipelines de données robustes et évolutifs en utilisant les services et les outils GCP tels que BigQuery, Pub/Sub, Cloud Storage, Cloud Composer etc.\n * Mettre en place des processus de transformation et de contrôle des données pour garantir leur qualité et leur cohérence. Utiliser des langages de programmation tels que SQL et Python et des outils tels que DBT pour effectuer des opérations de transformation et d\\'enrichissement des données, d\\'agrégation pour alimenter les cas d\\'usages métier,\n * Analyser les performances des pipelines de données, identifier les dérives et proposer des améliorations pour optimiser les temps de traitement et assurer une scalabilité adéquate,\n * Rédiger la documentation technique (spécifications, documents d\\'exploitation.) afin d\\'assurer la capitalisation,\n * Mettre en place des pipelines CI/CD pour automatiser le déploiement, les tests, et la gestion des développements,\n * Travailler en étroite collaboration avec les squads big data, les équipes de l\\'intégrateur et de la DSI ainsi que les métiers pour comprendre leurs besoins en matière de données,\n * Contribuer au support de production, à la correction des incidents et des anomalies, implémenter les évolutions fonctionnelles et techniques pour garantir la stabilité des traitements en production.\n* Télétravail de deux jours,\n * Un poste évolutif,\n * Un environnement de travail bienveillant et agréable.\nVous cumulez un minimum de 4 ans d\\'expérience, avec une expertise spécifique sur GCP sur des projets data intégrant d\\'importants volumes de données : \n * Solides compétences en développement et en mise en œuvre de pipelines de données sur GCP, en utilisant des services tels que Cloud Storage, BigQuery, Pub/Sub, Cloud Composer et des outils tels que DBT,\n * Maîtrise des langages de programmation tels que Python et SQL,\n * Maîtrise des pratiques DevOps, du CI/CD et de l\\'IaC,\n * Avoir travaillé dans le monde du retail est un réel plus...."
}]