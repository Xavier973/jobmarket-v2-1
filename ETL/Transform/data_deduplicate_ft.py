import os
import json

# D√©tection du chemin de base en fonction de l'environnement
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))  # Chemin du script actuel
BASE_PATH = os.path.dirname(os.path.dirname(SCRIPT_DIR))  # Remonte de 2 niveaux
DATA_TRANSFORM_FOLDER = os.path.join(BASE_PATH, "data", "transformed", "francetravail")
DATA_PROCESSED_FOLDER = os.path.join(BASE_PATH, "data", "processed", "francetravail")

print(f"üìÇ Dossier de donn√©es transform√©es : {DATA_TRANSFORM_FOLDER}")
print(f"üìÇ Dossier de donn√©es trait√©es : {DATA_PROCESSED_FOLDER}")

# Dictionnaire pour stocker les offres uniques bas√©es sur ft_reference
unique_offers = {}
duplicate_references = []

def load_json_file(file_path):
    """Charge un fichier JSON et retourne son contenu"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError:
        print(f"‚ùå Erreur de lecture du fichier {os.path.basename(file_path)}. Il pourrait √™tre corrompu.")
        return None

# Charger d'abord toutes les r√©f√©rences des fichiers trait√©s
processed_references = set()
for json_file in os.listdir(DATA_PROCESSED_FOLDER):
    if json_file.endswith(".json"):
        file_path = os.path.join(DATA_PROCESSED_FOLDER, json_file)
        data = load_json_file(file_path)
        if data:
            for job_offer in data:
                ft_ref = job_offer.get("ft_reference")
                if ft_ref:
                    processed_references.add(ft_ref)

print(f"üìä Nombre de r√©f√©rences d√©j√† trait√©es : {len(processed_references)}")

# Traitement des fichiers transform√©s
for json_file in os.listdir(DATA_TRANSFORM_FOLDER):
    if not json_file.endswith(".json"):
        continue
        
    file_path = os.path.join(DATA_TRANSFORM_FOLDER, json_file)
    data = load_json_file(file_path)
    if not data:
        continue

    # V√©rification des doublons et des r√©f√©rences d√©j√† trait√©es
    cleaned_data = []
    for job_offer in data:
        ft_ref = job_offer.get("ft_reference")
        if not ft_ref:
            continue

        # V√©rifier si l'offre est d√©j√† dans les fichiers trait√©s
        if ft_ref in processed_references:
            print(f"üîÑ Offre d√©j√† trait√©e : {ft_ref} (ignor√©e)")
            continue

        # V√©rifier les doublons dans le fichier courant
        if ft_ref in unique_offers:
            duplicate_references.append(ft_ref)
            print(f"üîÑ Doublon d√©tect√© : {ft_ref} (supprim√©)")
        else:
            unique_offers[ft_ref] = job_offer
            cleaned_data.append(job_offer)

    # √âcriture du fichier nettoy√©
    if cleaned_data:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(cleaned_data, f, ensure_ascii=False, indent=4)
        print(f"‚úÖ Fichier {json_file} nettoy√© : {len(cleaned_data)} offres conserv√©es")
    else:
        print(f"‚ÑπÔ∏è Fichier {json_file} vide apr√®s nettoyage, suppression...")
        os.remove(file_path)

# R√©sum√© du traitement
print("\nüìã R√©sum√© du traitement :")
print(f"- Nombre total d'offres uniques : {len(unique_offers)}")
print(f"- Nombre d'offres d√©j√† trait√©es : {len(processed_references)}")
if duplicate_references:
    print(f"- Nombre de doublons supprim√©s : {len(set(duplicate_references))}")
    print("\nR√©f√©rences en double supprim√©es :")
    print(", ".join(set(duplicate_references)))
else:
    print("- Aucun nouveau doublon trouv√©")

print("\nüöÄ Traitement termin√©.")
